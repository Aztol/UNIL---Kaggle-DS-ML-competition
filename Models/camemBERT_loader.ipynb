{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tokenizer\n",
    "!pip3 install sentencepiece\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer\n",
    "from tqdm import trange\n",
    "import nltk\n",
    "import tokenizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "MAX_LEN = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset, I selected only 5000 sample because of memory limitation\n",
    "df = pd.read_csv('training_data.csv').reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "# Mapping des valeurs de la colonne \"difficulty\"\n",
    "difficulty_mapping = {\n",
    "    'A1': 0,\n",
    "    'A2': 1,\n",
    "    'B1': 2,\n",
    "    'B2': 3,\n",
    "    'C1': 4,\n",
    "    'C2': 5\n",
    "}\n",
    "\n",
    "# Utiliser la fonction map pour encoder les valeurs\n",
    "df['difficulty_encoded'] = df['difficulty'].map(difficulty_mapping)\n",
    "\n",
    "unique_labels = df['difficulty_encoded'].unique()\n",
    "print(unique_labels)\n",
    "\n",
    "# Creates list of texts and labels\n",
    "text = df['sentence'].to_list()\n",
    "labels = df['difficulty_encoded'].to_list()  # Utilisez les labels encodés\n",
    "\n",
    "# Initialisation du tokenizer et du modèle\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\", do_lower_case=True)\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n",
    "model.to(device)\n",
    "\n",
    "# Définir une taille de lot initiale, si nécessaire\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "    # Tokenisation des données\n",
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True, truncation=True) for sent in text]\n",
    "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
    "\n",
    "    # Conversion en tenseurs\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels2 = torch.tensor(labels)\n",
    "\n",
    "    # Division en ensembles d'entraînement et de validation\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels2, attention_masks, random_state=42, test_size=0.2)\n",
    "\n",
    "    # Préparation des DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Définition de la fonction train_and_evaluate\n",
    "def train_and_evaluate(model, learning_rate, batch_size, num_epochs, device, df,train_dataloader, validation_dataloader, eps):\n",
    "\n",
    "    # Définition de l'optimiseur\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps)\n",
    "\n",
    "    # Boucle d'entraînement\n",
    "    for _ in trange(num_epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Évaluation du modèle\n",
    "        eval_accuracy = 0\n",
    "        nb_eval_steps = 0\n",
    "        model.eval()\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(np.argmax(logits, axis=1).flatten(), label_ids.flatten())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_accuracy /= nb_eval_steps\n",
    "\n",
    "    return eval_accuracy\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "learning_rates = [1e-5, 2e-5, 5e-5]\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs = [3, 5, 7]\n",
    "eps_values = [1e-8, 1e-7, 1e-6]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "df = pd.read_csv('training_data.csv')  # Assurez-vous d'avoir le dataframe 'df' chargé avec vos données\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = \"best_model.pt\"\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            for eps in eps_values:\n",
    "                accuracy = train_and_evaluate(model, lr, batch, epoch, device, df, train_dataloader, validation_dataloader, eps)\n",
    "                print(f\"LR: {lr}, Batch: {batch}, Epoch: {epoch}, Eps: {eps}, Accuracy: {accuracy}\")\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                                    best_accuracy = accuracy\n",
    "                                    torch.save(model.state_dict(), best_model_path)\n",
    "                                    print(f\"Meilleur modèle sauvegardé avec accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur modèle\n",
    "# Remplacez 'best_model_path' par le chemin de votre meilleur modèle sauvegardé\n",
    "model = CamembertForSequenceClassification.from_pretrained('best_model')\n",
    "model.to(device)\n",
    "\n",
    "# Charger le nouveau jeu de données\n",
    "new_df = pd.read_csv('unlabelled_test_data.csv')\n",
    "new_texts = new_df['sentence'].tolist()  # Assurez-vous que la colonne contient les phrases\n",
    "\n",
    "# Préparer les données pour le modèle\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)\n",
    "new_input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True, truncation=True) for sent in new_texts]\n",
    "new_attention_masks = [[float(i > 0) for i in seq] for seq in new_input_ids]\n",
    "\n",
    "# Convertir en tenseurs\n",
    "new_input_ids = torch.tensor(new_input_ids)\n",
    "new_attention_masks = torch.tensor(new_attention_masks)\n",
    "\n",
    "# Créer un DataLoader\n",
    "prediction_data = TensorDataset(new_input_ids, new_attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "# Prédiction\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.append(logits)\n",
    "\n",
    "# Convertir les prédictions en étiquettes de difficulté\n",
    "predicted_labels = [np.argmax(p, axis=1).flatten() for p in predictions]\n",
    "predicted_labels = np.concatenate(predicted_labels)\n",
    "\n",
    "# Créer un DataFrame pour le CSV\n",
    "output_df = pd.DataFrame({\n",
    "    'id': new_df.index,  # ou une autre colonne d'identification si disponible\n",
    "    'difficulty': [list(difficulty_mapping.keys())[list(difficulty_mapping.values()).index(label)] for label in predicted_labels]\n",
    "})\n",
    "\n",
    "# Enregistrer en CSV\n",
    "output_df.to_csv('predicted_difficulties.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
