{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPkI01nnW7fKDHbGpT51IhS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"afb09948982b4d8aad249165c497d7a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4196d4e731a40d6aeed20ed9e9435e0","IPY_MODEL_1da8db862da44b8aa37fe6941b8ab722","IPY_MODEL_85d13d5bb75d42f3896e3462b6ecbd36"],"layout":"IPY_MODEL_f1c83d6737a44a38b2b67aee20d773ef"}},"f4196d4e731a40d6aeed20ed9e9435e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_851c6b077cb749acb168f41d93d4ba82","placeholder":"​","style":"IPY_MODEL_1419b26fa1d244c6a34bb345ade3e794","value":"sentencepiece.bpe.model: 100%"}},"1da8db862da44b8aa37fe6941b8ab722":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e3e306828634dbea30e777944462421","max":810912,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48b288518cb4457ba01085692412a2f5","value":810912}},"85d13d5bb75d42f3896e3462b6ecbd36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d0c74e0d75f449d90dc0c40f3a9a22d","placeholder":"​","style":"IPY_MODEL_20378ac32ba24b02b0f487373cfd4b44","value":" 811k/811k [00:00&lt;00:00, 4.26MB/s]"}},"f1c83d6737a44a38b2b67aee20d773ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"851c6b077cb749acb168f41d93d4ba82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1419b26fa1d244c6a34bb345ade3e794":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e3e306828634dbea30e777944462421":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b288518cb4457ba01085692412a2f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d0c74e0d75f449d90dc0c40f3a9a22d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20378ac32ba24b02b0f487373cfd4b44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67ab2979afd149349f9ed3f43f8d056d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_738cd03033544597ad56169769a28f2d","IPY_MODEL_1d07fdf5e8464805ba4d974e4c15b469","IPY_MODEL_3490535cdfa8487a9f6255ac0188dd94"],"layout":"IPY_MODEL_4a28b329e9a64e37b4cf67a09bbd58df"}},"738cd03033544597ad56169769a28f2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_929680d57e8e407986e9bf9fb2b74ddc","placeholder":"​","style":"IPY_MODEL_3ffe4e7a2a544d42813190649ec579aa","value":"tokenizer.json: 100%"}},"1d07fdf5e8464805ba4d974e4c15b469":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a70c671612ce4531b51733f168720dc7","max":1395301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3dfb345b3c24262a117c86a7a345c19","value":1395301}},"3490535cdfa8487a9f6255ac0188dd94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d3cc233f39042508159afb6bce0485d","placeholder":"​","style":"IPY_MODEL_67b274dc757b4db6b072cde4b6c6968a","value":" 1.40M/1.40M [00:00&lt;00:00, 14.1MB/s]"}},"4a28b329e9a64e37b4cf67a09bbd58df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929680d57e8e407986e9bf9fb2b74ddc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffe4e7a2a544d42813190649ec579aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a70c671612ce4531b51733f168720dc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3dfb345b3c24262a117c86a7a345c19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d3cc233f39042508159afb6bce0485d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67b274dc757b4db6b072cde4b6c6968a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8fe412aa5ed4cf9adf9fff54a9a2223":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f71caf069e1f4bd4ac901e7743ea7faf","IPY_MODEL_43cdeae88c2f475998e45e9ab07c82c2","IPY_MODEL_4cc27c035253434dac246c882537a37c"],"layout":"IPY_MODEL_13db07503dc0427487b6105ad97daa1e"}},"f71caf069e1f4bd4ac901e7743ea7faf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a57057d96b364071bfd0b5fcf34b06fd","placeholder":"​","style":"IPY_MODEL_eec4d621cb7f4e05bda6a20c0bbd3cfe","value":"config.json: 100%"}},"43cdeae88c2f475998e45e9ab07c82c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79298147ac1a4124aa258bc42fbc63a9","max":508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19198025eedd4ebe9b393a5109ced00c","value":508}},"4cc27c035253434dac246c882537a37c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40b8b61513c54f6b87fef457e85b9926","placeholder":"​","style":"IPY_MODEL_2b4691c094d3451db0d04cf6bf4bb424","value":" 508/508 [00:00&lt;00:00, 20.4kB/s]"}},"13db07503dc0427487b6105ad97daa1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a57057d96b364071bfd0b5fcf34b06fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec4d621cb7f4e05bda6a20c0bbd3cfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79298147ac1a4124aa258bc42fbc63a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19198025eedd4ebe9b393a5109ced00c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40b8b61513c54f6b87fef457e85b9926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b4691c094d3451db0d04cf6bf4bb424":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16ecddb683a94a96b78a68ae62b30b02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43f63e538c914eefa1571089b3425a96","IPY_MODEL_40c79271d62c4c3982aa0d5f289be52a","IPY_MODEL_5f9e09d5656d44d2ac78d8be57a9e3cb"],"layout":"IPY_MODEL_3fa0b85810c7423b8abf3c14ae37e10a"}},"43f63e538c914eefa1571089b3425a96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75befb9725624130ba9c3bd2305192ed","placeholder":"​","style":"IPY_MODEL_2a0a422d4fb741009fc2a1ba611f3ca6","value":"model.safetensors: 100%"}},"40c79271d62c4c3982aa0d5f289be52a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f44a418854af4705b95f122483e850eb","max":445008750,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d371ce5cdd764f52a4453db8c5859128","value":445008750}},"5f9e09d5656d44d2ac78d8be57a9e3cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efb1804efe1a4ab8bb80b158e1227aaf","placeholder":"​","style":"IPY_MODEL_3010350985fe43ddb2a3d5d97de8c221","value":" 445M/445M [00:02&lt;00:00, 246MB/s]"}},"3fa0b85810c7423b8abf3c14ae37e10a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75befb9725624130ba9c3bd2305192ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a0a422d4fb741009fc2a1ba611f3ca6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f44a418854af4705b95f122483e850eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d371ce5cdd764f52a4453db8c5859128":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efb1804efe1a4ab8bb80b158e1227aaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3010350985fe43ddb2a3d5d97de8c221":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["afb09948982b4d8aad249165c497d7a8","f4196d4e731a40d6aeed20ed9e9435e0","1da8db862da44b8aa37fe6941b8ab722","85d13d5bb75d42f3896e3462b6ecbd36","f1c83d6737a44a38b2b67aee20d773ef","851c6b077cb749acb168f41d93d4ba82","1419b26fa1d244c6a34bb345ade3e794","7e3e306828634dbea30e777944462421","48b288518cb4457ba01085692412a2f5","3d0c74e0d75f449d90dc0c40f3a9a22d","20378ac32ba24b02b0f487373cfd4b44","67ab2979afd149349f9ed3f43f8d056d","738cd03033544597ad56169769a28f2d","1d07fdf5e8464805ba4d974e4c15b469","3490535cdfa8487a9f6255ac0188dd94","4a28b329e9a64e37b4cf67a09bbd58df","929680d57e8e407986e9bf9fb2b74ddc","3ffe4e7a2a544d42813190649ec579aa","a70c671612ce4531b51733f168720dc7","b3dfb345b3c24262a117c86a7a345c19","5d3cc233f39042508159afb6bce0485d","67b274dc757b4db6b072cde4b6c6968a","a8fe412aa5ed4cf9adf9fff54a9a2223","f71caf069e1f4bd4ac901e7743ea7faf","43cdeae88c2f475998e45e9ab07c82c2","4cc27c035253434dac246c882537a37c","13db07503dc0427487b6105ad97daa1e","a57057d96b364071bfd0b5fcf34b06fd","eec4d621cb7f4e05bda6a20c0bbd3cfe","79298147ac1a4124aa258bc42fbc63a9","19198025eedd4ebe9b393a5109ced00c","40b8b61513c54f6b87fef457e85b9926","2b4691c094d3451db0d04cf6bf4bb424","16ecddb683a94a96b78a68ae62b30b02","43f63e538c914eefa1571089b3425a96","40c79271d62c4c3982aa0d5f289be52a","5f9e09d5656d44d2ac78d8be57a9e3cb","3fa0b85810c7423b8abf3c14ae37e10a","75befb9725624130ba9c3bd2305192ed","2a0a422d4fb741009fc2a1ba611f3ca6","f44a418854af4705b95f122483e850eb","d371ce5cdd764f52a4453db8c5859128","efb1804efe1a4ab8bb80b158e1227aaf","3010350985fe43ddb2a3d5d97de8c221"]},"id":"nmAmhgdvV4MA","executionInfo":{"status":"ok","timestamp":1701263528659,"user_tz":-60,"elapsed":981225,"user":{"displayName":"laurent Sierro","userId":"01898903540875226616"}},"outputId":"6f2b6e14-bf35-46e9-c3c0-6e68fe40be9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[4 0 2 1 3 5]\n"]},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afb09948982b4d8aad249165c497d7a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ab2979afd149349f9ed3f43f8d056d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fe412aa5ed4cf9adf9fff54a9a2223"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","<ipython-input-2-3630afb4d3fa>:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_inputs = torch.tensor(train_inputs).to(device)\n","<ipython-input-2-3630afb4d3fa>:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_inputs = torch.tensor(validation_inputs).to(device)\n","<ipython-input-2-3630afb4d3fa>:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_lgth = torch.tensor(train_lgth).to(device)\n","<ipython-input-2-3630afb4d3fa>:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_lgth = torch.tensor(validation_lgth).to(device)\n","<ipython-input-2-3630afb4d3fa>:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_labels = torch.tensor(train_labels).to(device)\n","<ipython-input-2-3630afb4d3fa>:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_labels = torch.tensor(validation_labels).to(device)\n","<ipython-input-2-3630afb4d3fa>:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_masks = torch.tensor(train_masks).to(device)\n","<ipython-input-2-3630afb4d3fa>:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  validation_masks = torch.tensor(validation_masks).to(device)\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ecddb683a94a96b78a68ae62b30b02"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Train loss: 1.5818481507400672\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  10%|█         | 1/10 [01:35<14:20, 95.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.44479166666666664\n","Train loss: 1.2820983706663052\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  20%|██        | 2/10 [03:12<12:51, 96.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.478125\n","Train loss: 1.1358435655633607\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  30%|███       | 3/10 [04:49<11:14, 96.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5364583333333334\n","Train loss: 0.9868515074873964\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  40%|████      | 4/10 [06:25<09:37, 96.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5416666666666666\n","Train loss: 0.8404595196867982\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  50%|█████     | 5/10 [08:01<08:01, 96.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5552083333333333\n","Train loss: 0.6763753799411157\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  60%|██████    | 6/10 [09:37<06:25, 96.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.565625\n","Train loss: 0.5099224116032323\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  70%|███████   | 7/10 [11:13<04:48, 96.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5739583333333333\n","Train loss: 0.40602117984866104\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  80%|████████  | 8/10 [12:50<03:12, 96.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5520833333333334\n","Train loss: 0.2923111803053568\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  90%|█████████ | 9/10 [14:26<01:36, 96.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.55\n","Train loss: 0.2115765274541142\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [16:02<00:00, 96.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.5635416666666667\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["#!pip3 install tokenizer\n","#!pip3 install sentencepiece\n","import pandas as pd\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from transformers import AdamW\n","from transformers import CamembertTokenizer\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n","from transformers import CamembertForSequenceClassification, CamembertTokenizer\n","from tqdm import trange\n","import nltk\n","import tokenizer as tokenizer_2\n","import re\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","epochs = 10\n","MAX_LEN = 128\n","batch_size = 8\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the dataset, I selected only 5000 sample because of memory limitation\n","df = pd.read_csv('training_data_cleaned_length.csv').reset_index(drop=True)\n","df.head()\n","\n","# Mapping des valeurs de la colonne \"difficulty\"\n","difficulty_mapping = {\n","    'A1': 0,\n","    'A2': 1,\n","    'B1': 2,\n","    'B2': 3,\n","    'C1': 4,\n","    'C2': 5\n","}\n","\n","# Utiliser la fonction map pour encoder les valeurs\n","df['difficulty_encoded'] = df['difficulty'].map(difficulty_mapping)\n","\n","unique_labels = df['difficulty_encoded'].unique()\n","print(unique_labels)\n","\n","# Creates list of texts and labels\n","text = df['sentence'].to_list()\n","length = df['length']\n","labels = df['difficulty_encoded'].to_list()  # Utilisez les labels encodés\n","\n","# Utilisez le tokenizer Camembert\n","tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\", do_lower_case=True)\n","\n","\n","# Utilisez le tokenizer pour convertir les phrases en tokens\n","input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True, truncation=True) for sent in text]\n","\n","# Créez des masques d'attention\n","attention_masks = []\n","# Créez un masque de 1 pour chaque token suivi de 0 pour le padding\n","for seq in input_ids:\n","    seq_mask = [float(i > 0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","    # Convertissez les listes en tenseurs PyTorch\n","input_ids = torch.tensor(input_ids)\n","input_lgth = torch.tensor(length)\n","attention_masks = torch.tensor(attention_masks)\n","labels = torch.tensor(labels)\n","\n","# Créez un DataLoader pour gérer les lots de données\n","dataset = TensorDataset(input_ids, input_lgth, attention_masks, labels)\n","dataloader = DataLoader(dataset, batch_size=batch_size, sampler=RandomSampler(dataset))\n","\n","# Vous pouvez maintenant utiliser dataloader pour l'entraînement de votre modèle.\n","# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, validation_inputs, train_lgth, validation_lgth, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, input_lgth, labels, attention_masks,\n","                                                            random_state=42, test_size=0.2)\n","\n","\n","# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs).to(device)\n","validation_inputs = torch.tensor(validation_inputs).to(device)\n","train_lgth = torch.tensor(train_lgth).to(device)\n","validation_lgth = torch.tensor(validation_lgth).to(device)\n","train_labels = torch.tensor(train_labels).to(device)\n","validation_labels = torch.tensor(validation_labels).to(device)\n","train_masks = torch.tensor(train_masks).to(device)\n","validation_masks = torch.tensor(validation_masks).to(device)\n","\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_lgth, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_lgth, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n","model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=6)\n","model.to(device)\n","\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","from transformers import AdamW\n","from sklearn.metrics import accuracy_score\n","\n","# Define the optimizer and set the learning rate\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return accuracy_score(labels_flat, pred_flat)\n","\n","\n","# Store our loss and accuracy for plotting if we want to visualize training evolution per epochs after the training process\n","train_loss_set = []\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","    # Tracking variables for training\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    # Train the model\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        # Add batch to device CPU or GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_lgth, b_input_mask, b_labels = batch\n","        # Clear out the gradients (by default they accumulate)\n","        optimizer.zero_grad()\n","        # Forward pass\n","        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","        # Get loss value\n","        loss = outputs.loss\n","        # Add it to train loss list\n","        train_loss_set.append(loss.item())\n","        # Backward pass\n","        loss.backward()\n","        # Update parameters and take a step using the computed gradient\n","        optimizer.step()\n","# Update tracking variables\n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","\n","    # Tracking variables for validation\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    # Validation of the model\n","    model.eval()\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        # Add batch to device CPU or GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_lgth, b_input_mask, b_labels = batch\n","        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            outputs =  model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","            logits = outputs.logits\n","\n","        # Move logits and labels to CPU if GPU is used\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# Charger le nouveau jeu de données\n","# Remplacez 'new_data.csv' par le chemin de votre fichier de nouvelles phrases\n","new_df = pd.read_csv('unlabelled_test_data_length.csv')\n","new_texts = new_df['sentence'].tolist()  # Assurez-vous que la colonne contient les phrases\n","new_length = new_df['length']\n","# Préparer les données pour le modèle\n","tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)\n","new_input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True, truncation=True) for sent in new_texts]\n","new_attention_masks = [[float(i > 0) for i in seq] for seq in new_input_ids]\n","\n","# Convertir en tenseurs\n","new_input_ids = torch.tensor(new_input_ids)\n","new_input_lgth = torch.tensor(new_length)\n","new_attention_masks = torch.tensor(new_attention_masks)\n","\n","# Créer un DataLoader\n","prediction_data = TensorDataset(new_input_ids, new_input_lgth, new_attention_masks)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","# Prédiction\n","model.eval()\n","predictions = []\n","\n","for batch in prediction_dataloader:\n","    # Ajouter batch à GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_lgth, b_input_mask = batch\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n","\n","    logits = outputs.logits\n","    logits = logits.detach().cpu().numpy()\n","    predictions.append(logits)\n","\n","# Convertir les prédictions en étiquettes de difficulté\n","predicted_labels = [np.argmax(p, axis=1).flatten() for p in predictions]\n","predicted_labels = np.concatenate(predicted_labels)\n","\n","# Créer un DataFrame pour le CSV\n","output_df = pd.DataFrame({\n","    'id': new_df.index,  # ou une autre colonne d'identification si disponible\n","    'difficulty': [list(difficulty_mapping.keys())[list(difficulty_mapping.values()).index(label)] for label in predicted_labels]\n","})\n","\n","# Enregistrer en CSV\n","output_df.to_csv('predicted_difficulties5.csv', index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LpuM0WLEiZlg","executionInfo":{"status":"ok","timestamp":1701264788563,"user_tz":-60,"elapsed":8689,"user":{"displayName":"laurent Sierro","userId":"01898903540875226616"}},"outputId":"87536309-90d7-4555-c5fd-1a9df5ceb3dc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Nouvelle section"],"metadata":{"id":"mIRdP3ZFWdce"}}]}